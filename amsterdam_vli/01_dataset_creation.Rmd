---
title: "01_dataset_creation"
author: "Jay Chandra & Miguel √Ångel Armengol"
date: '`r format(Sys.Date(), "%B %d, %Y")`'
output:
  html_notebook:
    code_folding: hide
    number_sections: yes
    theme: flatly
    toc: yes
    toc_float: yes

knit: (function(inputFile, encoding) { rmarkdown::render(inputFile, encoding = encoding, output_file = paste0(substr(inputFile,1,nchar(inputFile)-4)," ",Sys.Date(),'.html')) })
---

# Environment

```{r message=FALSE, warning=FALSE}
library(bigrquery)
library(summarytools)
library(readr)
library(stringr)
library(sqldf)
library(dplyr)
library(tableone)
library(Hmisc)
library(caret)
library(plotly)
library(table1)

winsorize_x = function(x, cut = 0.05){
  cut_point_top <- quantile(x, 1 - cut, na.rm = T)
  cut_point_bottom <- quantile(x, cut, na.rm = T)
  i = which(x >= cut_point_top) 
  x[i] = cut_point_top
  j = which(x <= cut_point_bottom) 
  x[j] = cut_point_bottom
  return(x)
}
```


# Set up BigQuery related functions

This chunks also creates the run_query and get_sql function.

```{r setup, include=FALSE}
# Updated for our year
project_id <- "hst-953-2019"
options(httr_oauth_cache=FALSE)
# Function that takes in a sql command and runs it on bigquery
run_query <- function(query){
  data <- query_exec(query, project=project_id, use_legacy_sql=FALSE,max_pages = Inf)
  return(data)
}

# function for reading sql files
getSQL <- function(filepath){
  con = file(filepath, "r")
  sql.string <- ""

  while (TRUE){
    line <- readLines(con, n = 1)

    if ( length(line) == 0 ){
      break
    }

    line <- gsub("\\t", " ", line)

    if(grepl("--",line) == TRUE){
      line <- paste(sub("--","/*",line),"*/")
    }

    sql.string <- paste(sql.string, line)
  }

  close(con)
  return(sql.string)
}

'%!in%' <- function(x,y)!('%in%'(x,y))
```

# Loading queries and extracting the data

Loads all queries from the sql files in the extraction folder and runs them into RBigQuery to extract the data.

```{r}
amsterdam = run_query(getSQL("sql/amsterdam 2.sql" ))
```

```{r}
charlson = run_query(getSQL("sql/charlson 2.sql" ))
```


```{r}
sofa = run_query(getSQL("sql/sofa_final.sql" ))
```

```{r}
# Sepsis patients
sepsis <- run_query(getSQL('sql/sepsis.sql'))

# List of patients that are bleeding we want to exclude.
patient_inexcluded_icd9 <- run_query(getSQL('sql/patient_inexcluded_icd9.sql'))

#exclude patients who have recieved blood (I don't think we are doing this in eicu or mimic)
blood_infus <- run_query(getSQL('sql/blood_infus.sql'))

#72hr Fluid Data
fluid_72hrs <- run_query(getSQL('sql/fluid_72hrs.sql'))
```

# Exclusion criteria 1

Patients with sepsis (not bleeding) 

```{r}
# we want to include septic patients that are not bleeding.

print('Septic patients:')
nrow(sepsis)

print('Patients bleeding')
nrow(patient_inexcluded_icd9)

patient_included_vector <- sepsis[sepsis$admissionid %!in% patient_inexcluded_icd9$admissionid  ,]
patient_included <- data.frame(patientunitstayid=integer(length(patient_included_vector))) 
patient_included$patientunitstayid<-patient_included_vector

print('Septic patients that are not bleeding')
nrow(patient_included)

```


# Exclusion criteria 2 and JOIN

Exclusion criteria and dataset join (we need new datasets to address exclusion criteria)

```{r}
names(amsterdam)[1] <- "admissionid"
names(patient_included)[1] <- "admissionid"
selected_cohort<-inner_join(amsterdam,patient_included)
print('Patients >=16 years old:')
nrow(selected_cohort)

# We are using a left join to join them
vol_leak_index_dataset<-Reduce(function(...) merge(..., all.x=TRUE), list(
   selected_cohort
   ,fluid_72hrs
  ,charlson
))

# Removes Patients without intake data
vol_leak_index_dataset<-sqldf('
SELECT * FROM
vol_leak_index_dataset
WHERE intakes IS NOT null')

print('patients with fluid data')
nrow(vol_leak_index_dataset)

# Ensures fluid intake is greater than 0.
vol_leak_index_dataset<-sqldf('
SELECT * FROM
vol_leak_index_dataset
WHERE totalFluid > 0 ')

print('patients with positive fluid data')
nrow(vol_leak_index_dataset)
# Alter fluid data so all is positive
```

## Creating new variables

### Leaking Index

```{r}
vol_leak_index_dataset<-vol_leak_index_dataset%>%
  mutate(
    leaking_index=((mean_hct_24_36hrs - first_hct_6hrs) / totalFluid) * body_surface_area
  )  
```

### Addressing outliers

```{r eval=FALSE, include=FALSE}

# We are removing VLI outliers
# We are imputing anything outside the 95% interval
extreme_quants<-as.numeric(quantile(vol_leak_index_dataset$leaking_index, c(.025, .975),na.rm = T))

print('Number of patients with imputed data:')
length(which(vol_leak_index_dataset$leaking_index< extreme_quants[1] | vol_leak_index_dataset$leaking_index> extreme_quants[2]))

medLeak = median(vol_leak_index_dataset$leaking_index)
vol_leak_index_dataset$leaking_index[which(vol_leak_index_dataset$leaking_index< extreme_quants[1] | vol_leak_index_dataset$leaking_index> extreme_quants[2])] = medLeak
```

## SOFA Delta Description

|                   | Sofa day 2 No change | Sofa day 2 Increases | Sofa day 2 Decreases |
|-------------------|----------------------|----------------------|----------------------|
| High Sofa day 1   | Bad                  | Bad                  | Good                 |
| Medium Sofa day 1 | Bad                  | Bad                  | Good                 |
| Low Sofa day 1    | Good                 | Bad                  | Good                 |

0 Means GOOD Oucome, 1 means BAD Outcome

# Creating and Altering Variable
```{r}

# amsterdam$age_fixed = as.factor(amsterdam$age_fixed)
# amsterdam$gender[amsterdam$gender=='Male']<-1
# amsterdam$gender[amsterdam$gender=='Female']<-2
vol_leak_index_dataset$hosp_mortality = vol_leak_index_dataset$actualhospitalmortality * 1
vol_leak_index_dataset$q_leaking_index = as.factor(ntile(vol_leak_index_dataset$leaking_index,4))

# age_grouper = function(age) {
#   if (age >=18 & age <=39) {
#     group = '18-39'
#   } else if (age >=40 & age <= 49) {
#     group = '40-49'
#   } else if (age >=50 & age <=59) {
#     group = '50-59'
#   } else if (age >= 60 & age <= 69) {
#     group = '60-69'
#   } else if (age >= 70 & age <= 79) {
#     group = '70-79'
#   } else if (age >= 80) {
#     group = '80+'
#   } else {
#     group = NA
#   }
#   return(group)
# }
```

## Selecting/Imputing/removing data

```{r}
selected_df <- vol_leak_index_dataset%>%mutate(
  highest_q_leaking_index=if_else(
    q_leaking_index %in% c('3','4'),1,0
  ))%>%dplyr::select(
    admissionid
    ,hosp_mortality
    ,age_fixed
    ,gender
    ,charlson_score
    # ,apachescore
    # ,delta_sofa
    ,totalFluid
    ,totalFluid_72
    # ,highest_q_leaking_index
    ,q_leaking_index
    ,leaking_index
    ,first_hct_6hrs
    ,mean_hct_24_36hrs
  )
selected_df<-selected_df[complete.cases(selected_df),]

# selected_df$highest_q_leaking_index<-as.factor(selected_df$highest_q_leaking_index)

```

Amsterdam DB Cohort Information:

- Total patients in Amsterdam UMC DB: 23172
- After excluding patients without diagnosis of sepsis (n=9936): 13236
- After excluding patients with diagnosis of bleeding (n=4992): 8244
- After excluding patients who received blood/colloids (n=1757): 6487
- After excluding patients with missing CLI variables (n=4341): 2146
- After excluding patients with missing delta SOFA (n=1728): 418
(not using Apache because only 4869 patients in the entire dataset have apache)


# Table 1

```{r}
# table1(~ age_fixed + hosp_mortality + gender + charlson_score + apachescore + totalFluid + mean_hct_24_36hrs + first_hct_6hrs| q_leaking_index, data=selected_df)

table1(~ gender + age_fixed + hosp_mortality + totalFluid + mean_hct_24_36hrs + first_hct_6hrs + charlson_score| q_leaking_index, data=selected_df)
```

# 72hr Fluid Regression Analysis
```{r}
plot (selected_df$leaking_index, selected_df$totalFluid_72, xlab = 'leaking_index', ylab = 'fluid balance at 72hrs', main = 'Leaking Index as a predictor of Fluid Balance at 72 hrs')
linearMod <- lm(totalFluid_72 ~ q_leaking_index + totalFluid + charlson_score + gender, data=selected_df)
summary(linearMod)
```

# Train and test datasets creation

## Spliting de-identified data into testing and training, balanced version.

We want the data to be sampled randomly but always the same way and we want to be sure that train and test must be balanced.

```{r}
# Creating id for partition 
selected_df['id']<- seq.int(nrow(selected_df))
## set the seed to make our partition reproducible
set.seed(123)
# createDataPartition: "the random sampling is done within the levels of y when y is a factor in an attempt to balance the class distributions within the splits."
## 75% of the sample size
train_idx <- createDataPartition(as.factor(selected_df$hosp_mortality), times = 1, p = 0.75, list=F)

train <- selected_df[train_idx, ]
test <- selected_df[-train_idx, ]

#Checking outcome is actually balanced
round(prop.table(table(train$hosp_mortality)),2)
round(prop.table(table(test$hosp_mortality)),2)
```

## Separating datasets into outcome and exposures

```{r}
# train dataset
train_X<-train[, names(train)!= "hosp_mortality"]
train_Y<-train$hosp_mortality
  
# test dataset
test_X<-test[, names(test)!= "hosp_mortality"]
test_Y<-test$hosp_mortality 
```







