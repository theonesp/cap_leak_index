---
title: "02_model_training"
author: "Miguel √Ångel Armengol"
date: '`r format(Sys.Date(), "%B %d, %Y")`'
output:
  html_notebook:
    code_folding: hide
    number_sections: yes
    theme: flatly
    toc: yes
    toc_float: yes

knit: (function(inputFile, encoding) { rmarkdown::render(inputFile, encoding = encoding, output_file = paste0(substr(inputFile,1,nchar(inputFile)-4)," ",Sys.Date(),'.html')) })
---


# Environment

```{r message=FALSE, warning=FALSE}
require(foreign)
require(ggplot2)
require(MASS)
require(Hmisc)
require(reshape2)
require(caret)
require(boot)
require(pROC)
library(mlbench)
library(MLmetrics)
library(plotly)
library(gbm)
library(xgboost)
library(oddsratio)
library(ggplot2)
library(survey)
library(table1)
library(sandwich)
library(dplyr)
library(robustHD)
center_scale <- function(x) {
    scale(x, scale = FALSE)
}
```



# Addressing variables Distribution

## Distribution (Initial)

Below we can see how variables that will be used in our model are actually distributed, for those were it was required, we log transformed and standardized them.

```{r}
par(mfrow=c(1,2))
for (i in 2:ncol(selected_df)) {
colname<-names(selected_df[i])  
ifelse( nrow(unique(selected_df[i]))<10  
        ,barplot(table(selected_df[i]),main=colname,xlab=colname)
        ,hist(as.numeric(unlist(selected_df[,i]))
        ,main = paste("Histogram of" ,colname),xlab=colname)
         )
cat('Summary of ',colname,'\n',sep = '')  
cat(summary(selected_df[i]),'\n',sep = '|')
}
```

### Normalizing and standardizing variables

We applied log to the exposures that are skewed to log transform them.  
We also standardized (mean centered) the exposure.

```{r}
selected_df$apache_iv<-center_scale(log(selected_df$apache_iv))
selected_df$final_charlson_score<-log(selected_df$final_charlson_score+1)
```

We normalized and standardized apache_IV and final_charlson_score



# Distribution (After data cleansing) and variables creation

```{r}
par(mfrow=c(1,2))
for (i in 2:ncol(selected_df)) {
colname<-names(selected_df[i])  
ifelse( nrow(unique(selected_df[i]))<10  
        ,barplot(table(selected_df[i]),main=colname,xlab=colname)
        ,hist(as.numeric(unlist(selected_df[,i]))
        ,main = paste("Histogram of" ,colname),xlab=colname)
         )
cat('Summary of ',colname,'\n',sep = '')  
cat(summary(selected_df[i]),'\n',sep = '|')
}
```

We can see they are properly mean centered and standardized.

# Causal inference

## Propensity Score Matching - logistic regression

We are not including totalFluid in the model since it is highly correlated with VLI.

```{r}
#first of all we create a copy of the dataset
vli_prop_score_dataset<-selected_df

# Fit a propensity score model: logistic regression
# outcome is the treatment here.
ps_model <- glm( highest_q_leaking_index ~ 
                  apache_iv
                + age_fixed
                + gender
                + final_charlson_score
                , family=binomial()
                , data=vli_prop_score_dataset)

summary(ps_model)

OR_table<-as.data.frame(round(exp(cbind(OR=coef(ps_model), confint.default(ps_model))),2))
options(scipen=999) # disable scientific notation
OR_table

# Value of propensity score for each patient

pscore<-ps_model$fitted.values

treatment_pscore<-cbind(vli_prop_score_dataset%>%dplyr::select(highest_q_leaking_index),pscore)

ggplot(treatment_pscore, aes(x=pscore, fill = highest_q_leaking_index)) + 
  geom_histogram(alpha = 0.5,position="identity")+scale_fill_manual(name="Treatment: Prob of Highest Q of VLI",values=c("#1abc9c","#f1c40f")                                                                ,labels=c("Q1","Q4"))+theme_minimal()+ggtitle('Distribution of propensity score')
```


## Inverse probability weighting

```{r}
# create weight
hr_prop_score_dataset<-vli_prop_score_dataset%>%mutate(
  weight=if_else(highest_q_leaking_index==1 # TODO: what is the control group? People in q1,q2,q3
                 ,1/(pscore)
                 ,1/(1-pscore))
)

hr_prop_score_dataset$weight<-as.numeric(as.character(hr_prop_score_dataset$weight))

ggplot(hr_prop_score_dataset, aes(x = weight, fill = highest_q_leaking_index)) +
   geom_density(alpha = 0.5, colour = "grey50") +
   geom_rug() +
   scale_x_log10(breaks = c(1, 5, 10, 20, 40)) +   ggtitle("Distribution of inverse probability weights")+ scale_fill_manual(name="Treatment: Highest Quartile of VLI",values=c("#1abc9c","#f1c40f")
                                                                                                                            # ,labels=c("Q1","Q4")
                                                                                                                             )+theme_minimal()

# apply weights to data
weighted_data<-svydesign(ids = ~ patientunitstayid, data = hr_prop_score_dataset, weights = ~weight )

# weighted table 1

#weighedtable<- svyCreateTableOne( vars= c("age", "gender","final_charlson_score","predictedHospitalMortality"), strata = "highest_q_leaking_index", data = weighted_data, test = F)

# weighted table 1 wih standarized mean differences (SMD)
#as.data.frame(print(weighedtable,smd=T))
# Ignore SD and sample sizes, ONLY SMD is reliable

```

In tableone: Ignore SD and sample sizes, ONLY SMD is reliable


## Marginal structural modeling

### Hospital Mortality prediction

#### Relative risk with CI

```{r warning=FALSE}
# Obtaining causal relative risk. Weighed GLM

glm_model_rr<-glm(
  hosp_mortality ~ 
  as.factor(highest_q_leaking_index)  # treatment is the only exposure now
, weights = weight  
, family = binomial(link = log) # we are using the log link since we are interested in the relative risk
, data= hr_prop_score_dataset
)

# summary of the glm_model_rr final model
# beta Inverse probability weighting
betaiptw<-coef(glm_model_rr)

# to properly account for weighting, we are going to use asymptotic (sandwich) variance

SE<-sqrt(diag(vcovHC(glm_model_rr,type = "HC0"))) # getting the standard error.

# we get point estimate and CI for relative risk

beta_causal_relative_risk<-exp(betaiptw[2]) # we need to exponientiate since we logged before
lCI<-exp(betaiptw[2]-1.96*SE[2])
uCI<-exp(betaiptw[2]+1.96*SE[2])

beta_final<-as.data.frame(cbind(beta_causal_relative_risk, lCI, uCI
                    ))

rownames(beta_final)<-''

beta_final
```

#### Risk difference with CI

```{r warning=FALSE}
# Obtaining Obtaining risk difference with CI

glm_model_diff<-glm(
 hosp_mortality ~ 
  as.factor(highest_q_leaking_index) # treatment is the only exposure now
, weights = weight  
, family = binomial(link = 'identity') # we are using the identity link since we are interested in risk difference
, data= hr_prop_score_dataset
)

# summary of the glm_model_diff final model
# beta Inverse probability weighting
betaiptw<-coef(glm_model_diff)

# to properly account for weighting, we are going to use asymptotic (sandwich) variance

SE<-sqrt(diag(vcovHC(glm_model_diff,type = "HC0"))) # getting the standard error.

# we get point estimate and CI for relative risk

beta_risk_diference<-exp(betaiptw[2]) # we need to exponientiate since we logged before
lCI<-exp(betaiptw[2]-1.96*SE[2])
uCI<-exp(betaiptw[2]+1.96*SE[2])

beta_final<-as.data.frame(cbind(beta_risk_diference, lCI, uCI
                    ))

rownames(beta_final)<-''

beta_final
```


### Q Delta Sofa prediction

#### Relative risk with CI

```{r warning=FALSE}
# Obtaining causal relative risk. Weighed GLM

glm_model_rr<-glm(
  delta_sofa ~ 
  as.factor(highest_q_leaking_index)  # treatment is the only exposure now
, weights = weight  
, family = poisson(link = log) # we are using the log link since we are interested in the relative risk
, data= hr_prop_score_dataset
)

# summary of the glm_model_rr final model
# beta Inverse probability weighting
betaiptw<-coef(glm_model_rr)

# to properly account for weighting, we are going to use asymptotic (sandwich) variance

SE<-sqrt(diag(vcovHC(glm_model_rr,type = "HC0"))) # getting the standard error.

# we get point estimate and CI for relative risk

beta_causal_relative_risk<-exp(betaiptw[2]) # we need to exponientiate since we logged before
lCI<-exp(betaiptw[2]-1.96*SE[2])
uCI<-exp(betaiptw[2]+1.96*SE[2])

beta_final<-as.data.frame(cbind(beta_causal_relative_risk, lCI, uCI
                    ))

rownames(beta_final)<-''

beta_final
```

#### Risk difference with CI

```{r warning=FALSE}
# Obtaining Obtaining risk difference with CI

glm_model_diff<-glm(
  delta_sofa ~ 
  as.factor(highest_q_leaking_index) # treatment is the only exposure now
, weights = weight  
, family = poisson(link = 'identity') # we are using the identity link since we are interested in risk difference
, data= hr_prop_score_dataset
)

# summary of the glm_model_diff final model
# beta Inverse probability weighting
betaiptw<-coef(glm_model_diff)

# to properly account for weighting, we are going to use asymptotic (sandwich) variance

SE<-sqrt(diag(vcovHC(glm_model_diff,type = "HC0"))) # getting the standard error.

# we get point estimate and CI for relative risk

beta_risk_diference<-exp(betaiptw[2]) # we need to exponientiate since we logged before
lCI<-exp(betaiptw[2]-1.96*SE[2])
uCI<-exp(betaiptw[2]+1.96*SE[2])

beta_final<-as.data.frame(cbind(beta_risk_diference, lCI, uCI
                    ))

rownames(beta_final)<-''

beta_final
```

