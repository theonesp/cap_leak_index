---
title: "02_training_model"
author: "Miguel √Ångel Armengol de la Hoz"
date: "11/5/2019"
output:
  html_notebook:
    toc: TRUE
    theme: united
---

# Environment

```{r}
require(foreign)
require(ggplot2)
require(MASS)
require(Hmisc)
require(reshape2)
require(caret)
require(boot)
require(pROC)
library(mlbench)
library(MLmetrics )
library(plotly)
```

# Random Hyperparameter Search

The default method for optimizing tuning parameters in train is to use a grid search. This approach is usually effective but, in cases when there are many tuning parameters, it can be inefficient. An alternative is to use a combination of grid search and racing. Another is to use a random selection of tuning parameter combinations to cover the parameter space to a lesser extent.

_We can adress later the tuning parameters approach_

```{r}
fitControl <- trainControl(method = "repeatedcv",
                           number = 10,
                           repeats = 10,
                           classProbs = TRUE,
                           summaryFunction = twoClassSummary,
                           search = "random")
```

## Random selection of tuning parameter combinations

Here we are first adressing gbm and svmRadial methods.
We can also try with the rest.

```{r}
gbmFit <- train(
  actualhospitalmortality ~ 
                  age_fixed
                + gender
                + apachescore
                + final_charlson_score
                + q_leaking_index 
  ,data = train,
  method = "gbm",
  trControl = fitControl,
  verbose = FALSE,
  metric = "ROC" ## Specify which metric to optimize
)

svmFit <- train(
  actualhospitalmortality ~ 
                  age_fixed
                + gender
                + apachescore
                + final_charlson_score
                + q_leaking_index 
  ,
  data = train,
  method = "svmRadial",
  trControl = fitControl,
  preProc = c("center", "scale"),
  tuneLength = 8,
  metric = "ROC" ## Specify which metric to optimize
)


```

## Best models comprarision

```{r}
resamps <- resamples(list( GBM = gbmFit
                          ,SVM = svmFit))
summary(resamps)
```
We can see GBM is performing better.

# Selecting the best performing method

```{r}
# we save the best performing model and its name
selected_model<-gbmFit3
selected_model_name<-deparse(substitute(gbmFit3)) # extracts name as string from model
# we can plot what the best performing parameters are.
selected_model$bestTune
```

# Evaluating the predictor on our test dataset

```{r}

  
  
prediction_probabilities<-predict(selected_model, newdata = test,type = "prob") # We create the probabilities dataset using our best performing model.

final_predictions<-cbind(test_Y,prediction_probabilities) # we bind our prediction with the actual data
final_predictions<-rename(final_predictions, obs = test_Y)
final_predictions['pred']<-ifelse(final_predictions$ALIVE > .5 # we have set the threshold in .5 this can be optimized until best performance is achieved
                                  , 'ALIVE','EXPIRED'
)

# Setting proper data types
final_predictions$obs<-as.factor(final_predictions$obs)
final_predictions$pred<-as.factor(final_predictions$pred)
```

## Geting evaluation insights

```{r}
insights_1<-as.data.frame(twoClassSummary(final_predictions, lev = levels(final_predictions$obs)))
names(insights_1)<-selected_model_name
insights_1<-t(insights_1) # we traspose it for better merging it.

insights_2<-as.data.frame(prSummary(final_predictions, lev = levels(final_predictions$obs)))
names(insights_2)<-selected_model_name
insights_2<-t(insights_2) # we traspose it for better merging it.

evaluation_insights<-cbind(insights_1,insights_2)
evaluation_insights<-as.data.frame(evaluation_insights)
evaluation_insights<-round(evaluation_insights,2)
evaluation_insights$AUC<-NULL # We select the ROC from the first package so we remove this parameter

# we traspose the data for its representation
evaluation_insights<-t(evaluation_insights)
evaluation_insights<-as.data.frame(evaluation_insights)
evaluation_insights['Insights']<-rownames(evaluation_insights)

p <- plot_ly(
  data = evaluation_insights,
  x = ~gbmFit3,
  y = ~Insights,
  text = ~gbmFit3,
  textposition='auto',
  type = "bar") %>%
  layout( title = paste(selected_model_name,"Model Insights"))

p
```



