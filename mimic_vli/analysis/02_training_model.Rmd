---
title: "02_training_model"
author: "Miguel Ángel Armengol & Jay Chandra"
date: '`r format(Sys.Date(), "%B %d, %Y")`'
output:
  html_notebook:
    code_folding: hide
    number_sections: yes
    theme: flatly
    toc: yes
    toc_float: yes

knit: (function(inputFile, encoding) { rmarkdown::render(inputFile, encoding = encoding, output_file = paste0(substr(inputFile,1,nchar(inputFile)-4)," ",Sys.Date(),'.html')) })
---

# Environment

```{r}
library(mgcv)
library(ggplot2)
library(oddsratio)
```

# Plotting relationship

```{r}
ggplot(selected_df, aes(leaking_index,hosp_mortality) ) +
  stat_smooth()

ggplot(selected_df, aes(leaking_index,delta_sofa) ) +
  stat_smooth()

ggplot(selected_df_fluid, aes(leaking_index,fluid_balance_72) ) +
  stat_smooth()
```

# Overall cohort

## GAM analysis

GAM is a powerful and yet simple technique. Hence, the purpose of this post is to convince more data scientists to use GAM. Of course, GAM is no silver bullet, but it is a technique you should add to your arsenal. Here are three key reasons:

- Easy to interpret.
- Flexible predictor functions can uncover hidden patterns in the data.
- Regularization of predictor functions helps avoid overfitting.

In general, GAM has the interpretability advantages of GLMs where the contribution of each independent variable to the prediction is clearly encoded. However, it has substantially more flexibility because the relationships between independent and dependent variable are not assumed to be linear. In fact, we don’t have to know a priori what type of predictive functions we will eventually need. From an estimation standpoint, the use of regularized, nonparametric functions avoids the pitfalls of dealing with higher order polynomial terms in linear models. From an accuracy standpoint, GAMs are competitive with popular learning techniques.

Ref.:https://multithreaded.stitchfix.com/blog/2015/07/30/gam/

```{r}
# Build the model
model <- gam(hosp_mortality  ~
                         + s(leaking_index)
                         + age_fixed + gender + oasis + final_elixhauser_score
                         ,data=selected_df)
# Make predictions
predictions <- model %>% predict(selected_df)
# Model performance
data.frame(
  RMSE = RMSE(predictions, selected_df$hosp_mortality),
  R2 = R2(predictions, selected_df$hosp_mortality)
)

ggplot(selected_df, aes(leaking_index, hosp_mortality) ) +
  geom_point() +
  stat_smooth(method = gam, formula = y ~ s(x))
```

### Odds ratio

```{r}
or_gam(data = selected_df, model = model, pred = "leaking_index", 
                   values = c(min(selected_df$leaking_index), max(selected_df$leaking_index)))

or_gam(data = selected_df, model = model, pred = "age_fixed", 
                   values = c(min(selected_df$age_fixed), max(selected_df$age_fixed)))

or_gam(data = selected_df, model = model, pred = "oasis", 
                   values = c(min(selected_df$oasis), max(selected_df$oasis)))

or_gam(data = selected_df, model = model, pred = "final_elixhauser_score", 
                   values = c(min(selected_df$final_elixhauser_score), max(selected_df$final_elixhauser_score)))

or_gam(data = selected_df, model = model, pred = "gender", 
                   values = c('F', 'M') )
```


